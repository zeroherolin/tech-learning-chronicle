# AI编译器基础

## Compiler

- 常规编译器组成部分划分：前端（frontend）、优化器（optimizer）、后端（backend）

- IR的目的：保证编译器跨硬件平台

- 高阶IR：侧重抽象计算和控制流，捕获AI模型特性

- 低阶IR：注重硬件相关细节和代码生成，细粒度反映硬件相关优化和布局

- AI编译器：输入模型，硬件无关的前端优化，硬件相关的后端优化和代码生成

- 高阶IR（图IR）通常形式：有向无环图，节点代表计算操作 ，边代表数据依赖关系

- AI编译器挑战：异构硬件支持、自动优化、端到端性能

## LLVM

- LLVM IR与语言、指令集、类型系统无关，指令是SSA（Static Single Assignment，静态单赋值）形式

- LLVM由Clang前端、IR优化器和LLVM后端组成

- LLVM前端：词法分析器分解为Token，语法分析器识别程序语法结构，语义分析器组装表达式和语句函数等，输出AST

- LLVM后端：与目标平台无关的IR指令转换为目标平台相关的机器指令

- LLVM在AI中的作用：作为许多AI编译器的后端，提供优化Pass和代码生成

## GPGPU

- CUDA C/C++程序的标准编译过程是将运行在GPGPU设备上的函数编译为PTX代码

- Clang的CUDA前端在生成AST后，以两种模式分别预处理和解析输入的混合模式源文件 \
一种模式为主机生成LLVM IR，另一种模式为设备生成LLVM IR

- GPU编译优化方法建立在对GPU体系结构、存储结构基础上

## 开源AI编译器

### TVM

- 前端：Relay层负责将不同深度学习框架转换为统一表示，并在计算图层面进行优化

- 后端：TVM层负责具体算子的优化，采用计算和调度分离的设计

- 张量表达式语言：第一部分指定输出张量形状，第二部分描述张量中每个元素的计算规则

- 调度：指定如何执行计算，如访问数据的顺序、多线程并行的方式

- 张量化是指将低阶数据转换或映射为高阶数据的过程

- TVM通过张量intrinsic声明机制将目标硬件intrinsic与调度分离

- 针对大型搜索空间，TVM的XGBTuner调优器通过并行模拟退火算法寻找问题的最优解

### XLA

- XLA两级IR：HLO IR和低阶IR

- XLA是优化HLO并将其降级为机器码的后端框架

- XLA编译器通过编译子图并可缓存复用，减少短时操作的执行时间

- 集成：TensorFlow和JAX中使用，提供JIT/AOT模式

- 优化：融合操作、内存优化、并行执行

## MLIR

- 多级IR框架：允许多个抽象层级的Dialect表示程序

- Dialect：自定义操作和类型的集合，如Affine、Linalg

- Pass系统：转换和优化IR的机制，支持渐进式降低

- 在AI中的作用：桥接高阶框架（如PyTorch）和低阶后端（如LLVM）

- 优势：可重用组件、异构硬件支持、自定义优化

## IREE

- 端到端编译器：从MLIR输入生成可部署代码，支持多种后端（CPU、GPU、Vulkan）

- 运行时系统：轻量级VM执行编译后的模块

- 关键特性：AOT编译、动态形状支持、流式执行

- 在AI中的应用：移动/边缘部署、异构计算

- 与MLIR集成：直接从MLIR Dialect生成字节码

- 优势：跨平台、性能优化、低开销运行时

## Triton

- DSL for GPU：简化编写高性能内核，支持自动调优

- 语法：Python-like，用于定义块级操作

- 自动调优：搜索最佳 tiling、并行策略

- 在AI中的作用：自定义矩阵乘法、注意力等算子

- 与CUDA比较：更高抽象、更易优化复杂内核

- 集成：Triton可在PyTorch/TensorFlow中作为后端

***
🔙 [Go Back](README.md)
