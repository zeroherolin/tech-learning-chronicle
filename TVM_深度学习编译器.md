# TVM æ·±åº¦å­¦ä¹ ç¼–è¯‘å™¨

<img src="assets/tvm-logo-small.png" width=150/>

Code: [https://github.com/apache/tvm](https://github.com/apache/tvm) \
Doc Webpage: [https://tvm.apache.org/docs/](https://tvm.apache.org/docs/)

## æ„å»ºTVMç¯å¢ƒ

### æºç æ„å»º

```bash
conda create -n tvm-venv -c conda-forge \
    "llvmdev>=15" \
    "cmake>=3.24" \
    git \
    python=3.11

conda activate tvm-venv
conda install cython

git clone --recursive https://github.com/apache/tvm

mkdir build && cd build
cp ../cmake/config.cmake .

echo "set(CMAKE_BUILD_TYPE RelWithDebInfo)" >> config.cmake && \
echo "set(USE_LLVM \"llvm-config --ignore-libllvm --link-static\")" >> config.cmake && \
echo "set(HIDE_PRIVATE_SYMBOLS ON)" >> config.cmake && \
echo "set(USE_CUDA   ON)" >> config.cmake && \
echo "set(USE_METAL  OFF)" >> config.cmake && \
echo "set(USE_VULKAN OFF)" >> config.cmake && \
echo "set(USE_OPENCL OFF)" >> config.cmake && \
echo "set(USE_CUBLAS ON)" >> config.cmake && \
echo "set(USE_CUDNN  ON)" >> config.cmake && \
echo "set(USE_CUTLASS OFF)" >> config.cmake

cmake ..
cmake --build . --parallel $(nproc)

export TVM_LIBRARY_PATH=$(pwd)
pip install -e ../python
```

### æç®€å®‰è£…

```bash
pip install apache-tvm
```

### éªŒè¯å®‰è£…

```bash
# Check version
python -c "import tvm; print(tvm.__version__)"

# Locate TVM Python package
python -c "import tvm; print(tvm.__file__)"

# Confirm which TVM library is used
python -c "import tvm; print(tvm.ffi)"
# python -c "import tvm; print(tvm._ffi)"

# Check device detection
python -c "import tvm; print(tvm.metal().exist)"
python -c "import tvm; print(tvm.cuda().exist)"
python -c "import tvm; print(tvm.vulkan().exist)"

# Reflect TVM build option
python -c "import tvm; print('\n'.join(f'{k}: {v}' for k, v in tvm.support.libinfo().items()))"
```

## TVMæ€»ä½“æµ

<img src="assets/tvm_overall_flow.svg" width=600/>

- æ„å»ºæˆ–å¯¼å…¥æ¨¡å‹ï¼šæ„å»ºç¥ç»ç½‘ç»œæ¨¡å‹æˆ–ä»å…¶ä»–æ¡†æ¶ï¼ˆå¦‚PyTorchã€ONNXï¼‰å¯¼å…¥é¢„è®­ç»ƒæ¨¡å‹ï¼Œå¹¶åˆ›å»ºTVM IRModuleï¼Œå…¶ä¸­åŒ…å«ç¼–è¯‘æ‰€éœ€çš„æ‰€æœ‰ä¿¡æ¯ï¼ŒåŒ…æ‹¬ç”¨äºè®¡ç®—å›¾çš„é«˜çº§Relaxå‡½æ•°å’Œç”¨äºå¼ é‡ç¨‹åºçš„ä½çº§TensorIRå‡½æ•°

- æ‰§è¡Œå¯ç»„åˆä¼˜åŒ–ï¼šæ‰§è¡Œä¸€ç³»åˆ—ä¼˜åŒ–è½¬æ¢ï¼Œå¦‚å›¾å½¢ä¼˜åŒ–ã€å¼ é‡ç¨‹åºä¼˜åŒ–å’Œåº“è°ƒåº¦

- æ„å»ºå’Œé€šç”¨éƒ¨ç½²ï¼šå°†ä¼˜åŒ–åçš„æ¨¡å‹æ„å»ºä¸ºå¯éƒ¨ç½²åˆ°é€šç”¨è¿è¡Œæ—¶çš„æ¨¡å—ï¼Œå¹¶åœ¨ä¸åŒçš„è®¾å¤‡ï¼ˆå¦‚CPUã€GPUæˆ–å…¶ä»–åŠ é€Ÿå™¨ï¼‰ä¸Šæ‰§è¡Œ

## CPUåŸºç¡€æµ‹è¯•

```python
import tvm
from tvm import relax
from tvm.relax.frontend import nn
import numpy as np


class MLPModel(nn.Module):
    def __init__(self):
        super(MLPModel, self).__init__()
        self.fc1 = nn.Linear(784, 256)
        self.relu1 = nn.ReLU()
        self.fc2 = nn.Linear(256, 10)

    def forward(self, x):
        x = self.fc1(x)
        x = self.relu1(x)
        x = self.fc2(x)
        return x


mod, param_spec = MLPModel().export_tvm(
    spec={"forward": {"x": nn.spec.Tensor((1, 784), "float32")}}
)
mod.show()

mod = relax.get_pipeline("zero")(mod)

target = tvm.target.Target("llvm")
device = tvm.cpu()

ex = relax.build(mod, target)
vm = relax.VirtualMachine(ex, device)

params = []
for _, param_info in param_spec:
    param_np = np.random.rand(*param_info.shape).astype("float32")
    params.append(tvm.nd.array(param_np, device=device))

data = np.random.rand(1, 784).astype("float32")
tvm_data = tvm.nd.array(data, device=device)

out = vm["forward"](tvm_data, *params)
print(out.numpy())
```

## CUDAåŸºç¡€æµ‹è¯•

```python
import tvm
from tvm import te
import numpy as np

n = 16
A = te.placeholder((n, n), name="A")
B = te.placeholder((n, n), name="B")
k = te.reduce_axis((0, n), name="k")
C = te.compute((n, n), lambda i, j: te.sum(A[i, k] * B[k, j], axis=k), name="C")

func = te.create_prim_func([A, B, C])
mod = tvm.IRModule({"main": func})
mod.show()

sch = tvm.tir.Schedule(mod)
block = sch.get_block("C", func_name="main")
loops = sch.get_loops(block)

i, j = loops[:2]
io, ii = sch.split(i, factors=[None, 4])
jo, ji = sch.split(j, factors=[None, 4])
sch.bind(io, "blockIdx.x")
sch.bind(jo, "blockIdx.y")
sch.bind(ii, "threadIdx.x")
sch.bind(ji, "threadIdx.y")

target = tvm.target.Target("cuda")
device = tvm.cuda(0)

cuda_mod = tvm.build(sch.mod, target=target)

a_np = np.ones((n, n)).astype(np.float32)
b_np = np.ones((n, n)).astype(np.float32)
a = tvm.nd.array(a_np, device=device)
b = tvm.nd.array(b_np, device=device)
c = tvm.nd.array(b_np, device=device)

cuda_mod(a, b, c)
print(c.numpy())
```

***
ğŸ”™ [Go Back](README.md)
