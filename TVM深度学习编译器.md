# TVMæ·±åº¦å­¦ä¹ ç¼–è¯‘å™¨

<img src="assets/tvm-logo-small.png" width=150/>

Code: [https://github.com/apache/tvm](https://github.com/apache/tvm) \
Doc Webpage: [https://tvm.apache.org/docs/](https://tvm.apache.org/docs/)

## æ„å»ºTVMç¯å¢ƒ

### æºç æ„å»º

```bash
conda create -n tvm-venv -c conda-forge \
    "llvmdev>=15" \
    "cmake>=3.24" \
    git \
    python=3.11

conda activate tvm-venv
conda install cython

git clone --recursive https://github.com/apache/tvm

mkdir build && cd build
cp ../cmake/config.cmake .

echo "set(CMAKE_BUILD_TYPE RelWithDebInfo)" >> config.cmake && \
echo "set(USE_LLVM \"llvm-config --ignore-libllvm --link-static\")" >> config.cmake && \
echo "set(HIDE_PRIVATE_SYMBOLS ON)" >> config.cmake && \
echo "set(USE_CUDA   ON)" >> config.cmake && \
echo "set(USE_METAL  OFF)" >> config.cmake && \
echo "set(USE_VULKAN OFF)" >> config.cmake && \
echo "set(USE_OPENCL OFF)" >> config.cmake && \
echo "set(USE_CUBLAS ON)" >> config.cmake && \
echo "set(USE_CUDNN  ON)" >> config.cmake && \
echo "set(USE_CUTLASS OFF)" >> config.cmake

cmake ..
cmake --build . --parallel $(nproc)

export TVM_LIBRARY_PATH=$(pwd)
pip install -e ../python
```

### æç®€å®‰è£…

```bash
pip install apache-tvm
```

### éªŒè¯å®‰è£…

```bash
# Check version
python -c "import tvm; print(tvm.__version__)"

# Locate TVM Python package
python -c "import tvm; print(tvm.__file__)"

# Confirm which TVM library is used
python -c "import tvm; print(tvm.ffi)"
# python -c "import tvm; print(tvm._ffi)"

# Check device detection
python -c "import tvm; print(tvm.metal().exist)"
python -c "import tvm; print(tvm.cuda().exist)"
python -c "import tvm; print(tvm.vulkan().exist)"

# Reflect TVM build option
python -c "import tvm; print('\n'.join(f'{k}: {v}' for k, v in tvm.support.libinfo().items()))"
```

## TVMæ€»ä½“æµ

<img src="assets/tvm_overall_flow.svg" width=600/>

- æ„å»ºæˆ–å¯¼å…¥æ¨¡å‹ï¼šæ„å»ºç¥ç»ç½‘ç»œæ¨¡å‹æˆ–ä»å…¶ä»–æ¡†æ¶ï¼ˆå¦‚PyTorchã€ONNXï¼‰å¯¼å…¥é¢„è®­ç»ƒæ¨¡å‹ï¼Œå¹¶åˆ›å»ºTVM IRModuleï¼Œå…¶ä¸­åŒ…å«ç¼–è¯‘æ‰€éœ€çš„æ‰€æœ‰ä¿¡æ¯ï¼ŒåŒ…æ‹¬ç”¨äºè®¡ç®—å›¾çš„é«˜çº§Relaxå‡½æ•°å’Œç”¨äºå¼ é‡ç¨‹åºçš„ä½çº§TensorIRå‡½æ•°

- æ‰§è¡Œå¯ç»„åˆä¼˜åŒ–ï¼šæ‰§è¡Œä¸€ç³»åˆ—ä¼˜åŒ–è½¬æ¢ï¼Œå¦‚å›¾å½¢ä¼˜åŒ–ã€å¼ é‡ç¨‹åºä¼˜åŒ–å’Œåº“è°ƒåº¦

- æ„å»ºå’Œé€šç”¨éƒ¨ç½²ï¼šå°†ä¼˜åŒ–åçš„æ¨¡å‹æ„å»ºä¸ºå¯éƒ¨ç½²åˆ°é€šç”¨è¿è¡Œæ—¶çš„æ¨¡å—ï¼Œå¹¶åœ¨ä¸åŒçš„è®¾å¤‡ï¼ˆå¦‚CPUã€GPUæˆ–å…¶ä»–åŠ é€Ÿå™¨ï¼‰ä¸Šæ‰§è¡Œ

## Relax IR

- Relayæ˜¯TVMçš„é«˜çº§ä¸­é—´è¡¨ç¤ºï¼Œç”¨äºè¡¨ç¤ºæ·±åº¦å­¦ä¹ æ¨¡å‹å’Œè®¡ç®—å›¾
- Relayé‡‡ç”¨å‡½æ•°å¼ç¼–ç¨‹èŒƒå¼ï¼Œæ”¯æŒå„ç§ä¼˜åŒ–å˜æ¢ï¼Œå¦‚å›¾ä¼˜åŒ–ã€é‡åŒ–ç­‰
- Relayå…è®¸ä»å‰ç«¯æ¡†æ¶ï¼ˆå¦‚PyTorchã€ONNXï¼‰å¯¼å…¥æ¨¡å‹ï¼Œå¹¶è½¬æ¢ä¸ºå¯ä¼˜åŒ–çš„å½¢å¼

Relayç°å·²è¢«Relaxå–ä»£

- Relaxé‡‡ç”¨ç¬¦å·å½¢çŠ¶è¡¨ç¤ºå¼ é‡ç»´åº¦ï¼Œæ”¯æŒå…¨å±€è·Ÿè¸ªåŠ¨æ€å½¢çŠ¶å…³ç³»
- Relaxæ”¯æŒä»é«˜çº§ç¥ç»ç½‘ç»œå±‚åˆ°ä½çº§å¼ é‡æ“ä½œçš„è·¨çº§æŠ½è±¡ï¼Œå®ç°è·¨å±‚çº§ä¼˜åŒ–
- Relaxæä¾›å¯ç»„åˆçš„å˜æ¢æ¡†æ¶ï¼Œå¯é€‰æ‹©æ€§åœ°åº”ç”¨äºæ¨¡å‹çš„ä¸åŒç»„ä»¶ï¼Œæä¾›çµæ´»çš„è‡ªå®šä¹‰å’Œä¼˜åŒ–é€‰é¡¹

### Relaxç¤ºä¾‹

```python
import tvm
from tvm import relax

x = relax.Var("x", relax.TensorStructInfo([1, 784], "float32"))
w1 = relax.Var("w1", relax.TensorStructInfo([256, 784], "float32"))
b1 = relax.Var("b1", relax.TensorStructInfo([256], "float32"))
w2 = relax.Var("w2", relax.TensorStructInfo([10, 256], "float32"))
b2 = relax.Var("b2", relax.TensorStructInfo([10], "float32"))

hidden = relax.op.add(relax.op.matmul(x, relax.op.permute_dims(w1)), b1)
hidden = relax.op.nn.relu(hidden)
output = relax.op.add(relax.op.matmul(hidden, relax.op.permute_dims(w2)), b2)

func = relax.Function([x, w1, b1, w2, b2], output, ret_struct_info=relax.TensorStructInfo([1, 10], "float32"))
mod = tvm.IRModule.from_expr(func)
mod.show()
```

## ç®—å­å®šä¹‰

- åœ¨TVMä¸­ï¼Œå¯ä»¥ä½¿ç”¨Tensor Expressionå®šä¹‰è‡ªå®šä¹‰ç®—å­
- TEæä¾›äº†ä¸€ç§å£°æ˜å¼çš„æ–¹å¼æ¥æè¿°è®¡ç®—ï¼Œç„¶åé€šè¿‡è°ƒåº¦ï¼ˆScheduleï¼‰æ¥ä¼˜åŒ–å…¶å®ç°

### ç®—å­å®šä¹‰ç¤ºä¾‹

ä¸€ä¸ªç®€å•çš„å¼ é‡åŠ 

```python
import tvm
from tvm import te
import numpy as np

n = 10
A = te.placeholder((n,), name="A")
B = te.compute((n,), lambda i: A[i] + 1.0, name="B")
func = te.create_prim_func([A, B])
mod = tvm.build(func, target="llvm")
dev = tvm.cpu(0)
a = tvm.nd.array(np.arange(n).astype("float32"), dev)
b = tvm.nd.array(np.zeros(n, dtype="float32"), dev)
mod(a, b)
print(b.numpy())
```

## éƒ¨ç½²ç¤ºä¾‹

- TVMæ”¯æŒå°†ä¼˜åŒ–åçš„æ¨¡å‹éƒ¨ç½²åˆ°å„ç§åç«¯ï¼Œéƒ¨ç½²è¿‡ç¨‹åŒ…æ‹¬æ„å»ºå¯æ‰§è¡Œæ¨¡å—å¹¶åœ¨è¿è¡Œæ—¶æ‰§è¡Œ

### CPUåŸºç¡€éƒ¨ç½²

æ„é€ ä¸€ä¸ªç®€å•çš„MLPModel

```python
import tvm
from tvm import relax
from tvm.relax.frontend import nn
import numpy as np


class MLPModel(nn.Module):
    def __init__(self):
        super(MLPModel, self).__init__()
        self.fc1 = nn.Linear(784, 256)
        self.relu1 = nn.ReLU()
        self.fc2 = nn.Linear(256, 10)

    def forward(self, x):
        x = self.fc1(x)
        x = self.relu1(x)
        x = self.fc2(x)
        return x


mod, param_spec = MLPModel().export_tvm(
    spec={"forward": {"x": nn.spec.Tensor((1, 784), "float32")}}
)

mod = relax.get_pipeline("zero")(mod)
mod.show()

target = tvm.target.Target("llvm")
device = tvm.cpu()

ex = relax.build(mod, target)
vm = relax.VirtualMachine(ex, device)

params = []
for _, param_info in param_spec:
    param_np = np.random.rand(*param_info.shape).astype("float32")
    params.append(tvm.nd.array(param_np, device=device))

data = np.random.rand(1, 784).astype("float32")
tvm_data = tvm.nd.array(data, device=device)

out = vm["forward"](tvm_data, *params)
print(out.numpy())
```

### CUDAåŸºç¡€éƒ¨ç½²

ä½¿ç”¨dlightè‡ªåŠ¨ä¼˜åŒ–GPU

```python
import tvm
from tvm import relax
from tvm.relax.frontend import nn
import numpy as np
from tvm import dlight as dl


class MLPModel(nn.Module):
    def __init__(self):
        super(MLPModel, self).__init__()
        self.fc1 = nn.Linear(784, 256)
        self.relu1 = nn.ReLU()
        self.fc2 = nn.Linear(256, 10)

    def forward(self, x):
        x = self.fc1(x)
        x = self.relu1(x)
        x = self.fc2(x)
        return x


mod, param_spec = MLPModel().export_tvm(
    spec={"forward": {"x": nn.spec.Tensor((1, 784), "float32")}}
)

mod = relax.get_pipeline("zero")(mod)

target = tvm.target.Target("cuda")
device = tvm.cuda()

with target:
    mod = dl.ApplyDefaultSchedule(
        dl.gpu.Matmul(),
        dl.gpu.GEMV(),
        dl.gpu.Reduction(),
        dl.gpu.GeneralReduction(),
        dl.gpu.Fallback(),
    )(mod)
mod.show()

ex = relax.build(mod, target)
vm = relax.VirtualMachine(ex, device)

params = []
for _, param_info in param_spec:
    param_np = np.random.rand(*param_info.shape).astype("float32")
    params.append(tvm.nd.array(param_np, device=device))

data = np.random.rand(1, 784).astype("float32")
tvm_data = tvm.nd.array(data, device=device)

out = vm["forward"](tvm_data, *params)
print(out.numpy())
```

ä¹Ÿå¯ä»¥æ‰‹åŠ¨ç»‘å®š

```python
with target:
    new_mod = tvm.IRModule()
    for gv, func in mod.functions.items():
        if not isinstance(func, tir.PrimFunc):
            new_mod[gv] = func
            continue
        sch = tir.Schedule(func)
        root = sch.get_block("root")
        for block in sch.get_child_blocks(root):
            loops = sch.get_loops(block)
            if len(loops) >= 1:
                sch.bind(loops[0], "blockIdx.x")
            if len(loops) >= 2:
                sch.bind(loops[1], "threadIdx.x")
        new_mod[gv] = sch.mod["main"]
    mod = new_mod
```

å†æ¥ä¸€ä¸ªçŸ©é˜µä¹˜ç®€å•ç¤ºä¾‹

```python
import tvm
from tvm import te
import numpy as np

n = 16
A = te.placeholder((n, n), name="A")
B = te.placeholder((n, n), name="B")
k = te.reduce_axis((0, n), name="k")
C = te.compute((n, n), lambda i, j: te.sum(A[i, k] * B[k, j], axis=k), name="C")

func = te.create_prim_func([A, B, C])
mod = tvm.IRModule({"main": func})
mod.show()

sch = tvm.tir.Schedule(mod)
block = sch.get_block("C", func_name="main")
loops = sch.get_loops(block)

i, j = loops[:2]
io, ii = sch.split(i, factors=[None, 4])
jo, ji = sch.split(j, factors=[None, 4])
sch.bind(io, "blockIdx.x")
sch.bind(jo, "blockIdx.y")
sch.bind(ii, "threadIdx.x")
sch.bind(ji, "threadIdx.y")

target = tvm.target.Target("cuda")
device = tvm.cuda(0)

cuda_mod = tvm.build(sch.mod, target=target)

a_np = np.ones((n, n)).astype(np.float32)
b_np = np.ones((n, n)).astype(np.float32)
a = tvm.nd.array(a_np, device=device)
b = tvm.nd.array(b_np, device=device)
c = tvm.nd.array(b_np, device=device)

cuda_mod(a, b, c)
print(c.numpy())
```

## AutoTVM

- AutoTVMæ˜¯TVMçš„è‡ªåŠ¨è°ƒä¼˜æ¡†æ¶ï¼Œç”¨äºä¼˜åŒ–å¼ é‡è¡¨è¾¾å¼çš„æ€§èƒ½
- AutoTVMé€šè¿‡æœç´¢æœ€ä½³çš„è°ƒåº¦ç­–ç•¥ï¼ˆå¦‚å¾ªç¯ tilingã€å¹¶è¡ŒåŒ–ï¼‰æ¥ç”Ÿæˆé«˜æ•ˆçš„å†…æ ¸ä»£ç 
- AutoTVMæ”¯æŒåŸºäºæ¨¡æ¿çš„è°ƒä¼˜ï¼Œå¹¶å¯ä»¥ä½¿ç”¨æœºå™¨å­¦ä¹ æ¨¡å‹åŠ é€Ÿæœç´¢è¿‡ç¨‹

AutoTVMç°å·²è¢«Meta-Scheduleå–ä»£

- Meta-Scheduleä½¿ç”¨åŸºäºè§„åˆ™çš„è°ƒåº¦å’Œè‡ªåŠ¨è°ƒä¼˜ï¼Œæä¾›æ›´å¤§æœç´¢ç©ºé—´å’Œæ›´å¥½æ€§èƒ½ï¼Œæ— éœ€æ‰‹åŠ¨å®šä¹‰æ¨¡æ¿
- Meta-Scheduleå†…ç½®æˆæœ¬æ¨¡å‹å’Œè°ƒä¼˜ç®—æ³•ï¼Œæ”¯æŒæœºå™¨å­¦ä¹ é©±åŠ¨çš„æœç´¢åŠ é€Ÿ
- Meta-Scheduleæ›´å¥½åœ°å¤„ç†åŠ¨æ€å½¢çŠ¶å’Œæ›´å¤æ‚çš„IRï¼Œæ”¯æŒTIRçš„ç«¯åˆ°ç«¯ä¼˜åŒ–
- Meta-Scheduleå‡å°‘äº†æ‰‹åŠ¨é…ç½®éœ€æ±‚ï¼Œä½¿è°ƒä¼˜è¿‡ç¨‹æ›´é«˜æ•ˆå’Œç”¨æˆ·å‹å¥½

### Meta-Scheduleç¤ºä¾‹

ä¼˜åŒ–ä¸€ä¸ªçŸ©é˜µä¹˜

```python
import tvm
from tvm import meta_schedule as ms
from tvm.script import tir as T
import numpy as np
import tvm.testing
from tvm.meta_schedule import tir_integration

@T.prim_func(private=True)
def matmul(a: T.handle, b: T.handle, c: T.handle) -> None:
    A = T.match_buffer(a, (128, 128), "float32")
    B = T.match_buffer(b, (128, 128), "float32")
    C = T.match_buffer(c, (128, 128), "float32")
    for i, j, k in T.grid(128, 128, 128):
        with T.block("update"):
            vi, vj, vk = T.axis.remap("SSR", [i, j, k])
            with T.init():
                C[vi, vj] = 0.0
            C[vi, vj] = C[vi, vj] + A[vi, vk] * B[vj, vk]

target = tvm.target.Target("llvm -num-cores=4")

# Tuning
database = ms.tune_tir(
    mod=matmul,
    target=target,
    max_trials_global=64,
    num_trials_per_iter=2,
    work_dir="./tune_tmp",
    runner=ms.runner.LocalRunner(
        evaluator_config=ms.runner.EvaluatorConfig(
            number=3,
            repeat=1,
        )
    ),
    cost_model=ms.cost_model.RandomModel(),
)

sch = tir_integration.compile_tir(database, matmul, target)
print(sch.trace)

# Build the tuned function
with tvm.transform.PassContext(opt_level=3):
    rt = tvm.build(sch.mod, target=target)

# Test
a_np = np.random.uniform(size=(128, 128)).astype(np.float32)
b_np = np.random.uniform(size=(128, 128)).astype(np.float32)
c_np = a_np.dot(b_np.transpose())

dev = tvm.cpu()
a = tvm.nd.array(a_np, device=dev)
b = tvm.nd.array(b_np, device=dev)
c = tvm.nd.array(np.zeros((128, 128), dtype="float32"), device=dev)

f_timer = rt.time_evaluator("main", dev, number=10)
print("Latency:", f_timer(a, b, c).mean)

tvm.testing.assert_allclose(c.numpy(), c_np, rtol=1e-5)
```

***
ğŸ”™ [Go Back](README.md)
