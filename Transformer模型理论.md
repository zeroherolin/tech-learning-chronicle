# Transformeræ¨¡å‹ç†è®º

Paper: [Attention Is All You Need](https://arxiv.org/pdf/1706.03762)

## Transformeræ¨¡å‹æ¡†æ¶

<img src="assets/transformer-arch.png" height=500/> <img src="assets/transformer-layers.png" height=500/>

- æ•´ä½“ç»“æ„ï¼šEncoder-Decoder

## è¾“å…¥åµŒå…¥ï¼ˆInput Embeddingï¼‰

- è¾“å…¥åºåˆ—Xæ˜¯ç¦»æ•£ç¬¦å·ï¼ˆå¦‚å•è¯IDï¼‰
- é€šè¿‡åµŒå…¥çŸ©é˜µæŸ¥è¡¨ï¼Œå°†æ¯ä¸ªç¬¦å·xáµ¢æ˜ å°„ä¸ºç¨ å¯†å‘é‡eáµ¢ï¼ˆç»´åº¦d_modelï¼‰

```
X = (xâ‚, xâ‚‚, ..., xâ‚™) --> E = (eâ‚, eâ‚‚, ..., eâ‚™)
eáµ¢ = EmbeddingLookup(xáµ¢)    # eáµ¢ âˆˆ â„^(d_model)
```

## ä½ç½®ç¼–ç ï¼ˆPositional Encodingï¼‰

- ä¸ºåºåˆ—ä½ç½®æ³¨å…¥é¡ºåºä¿¡æ¯ï¼Œé¿å…è‡ªæ³¨æ„åŠ›ä¸¢å¤±ä½ç½®å…³ç³»
- ä½¿ç”¨æ­£å¼¦/ä½™å¼¦å‡½æ•°ç”Ÿæˆå›ºå®šç¼–ç ï¼ˆéå¯å­¦ä¹ ï¼‰

```
PE(pos, 2i) = sin(pos / 10000^(2i / d_model))
PE(pos, 2i+1) = cos(pos / 10000^(2i / d_model))

Z = E + PE    # Z âˆˆ â„^(n Ã— d_model)
```

## è‡ªæ³¨æ„åŠ›ï¼ˆSelf-Attentionï¼‰

- è®¡ç®—æ¯ä¸ªä½ç½®ä¸å…¶ä»–ä½ç½®çš„å…³è”æƒé‡ï¼ˆsoftmaxå½’ä¸€åŒ–ï¼‰
- ç¼©æ”¾å› å­âˆšd_ké˜²æ­¢ç‚¹ç§¯è¿‡å¤§å¯¼è‡´æ¢¯åº¦æ¶ˆå¤±
- Maskåœ¨è§£ç å™¨ä¸­å±è”½æœªæ¥ä½ç½®ï¼ˆç¼–ç å™¨é€šå¸¸ä¸é€‚ç”¨ï¼‰

```
Q = X * W^Q    # W^Q âˆˆ â„^(d_model Ã— d_k)    Q âˆˆ â„^(n Ã— d_k)
K = X * W^K    # W^K âˆˆ â„^(d_model Ã— d_k)    K âˆˆ â„^(n Ã— d_k)
V = X * W^V    # W^V âˆˆ â„^(d_model Ã— d_v)    V âˆˆ â„^(n Ã— d_v)

Scores = Q * K^T    # Scores âˆˆ â„^(n Ã— n)
Scaled_Scores = Scores / âˆšd_k    # Scaled_Scores âˆˆ â„^(n Ã— n)

Attention(Q, K, V) = softmax((Q * K^T) / âˆšd_k + Mask) * V    # Output âˆˆ â„^{n Ã— d_v}
```

## å¤šå¤´æ³¨æ„åŠ›ï¼ˆMulti-Head Attentionï¼‰

- å¹¶è¡Œæ‰§è¡Œhç»„è‡ªæ³¨æ„åŠ›ï¼Œæ•æ‰ä¸åŒå­ç©ºé—´çš„ç‰¹å¾
- æ‹¼æ¥åé€šè¿‡çº¿æ€§å±‚W^Oèåˆä¿¡æ¯å¹¶æ¢å¤ç»´åº¦

```
head_i = Attention(X * W_i^Q, X * W_i^K , X * W_i^V)    # head_i âˆˆ â„^(n Ã— d_v)

Concat_Heads = [head_1; head_2; ...; head_h]    # Concat_Heads âˆˆ â„^(n Ã— (h * d_v))

MultiHeadOutput = Concat_Heads * W^O
# W^O âˆˆ â„^(d_model Ã— d_model)    MultiHeadOutput âˆˆ â„^(n Ã— d_model)
```

## ä½ç½®å¼å‰é¦ˆç¥ç»ç½‘ç»œï¼ˆPosition-wise Feed-Forward Network - FFNï¼‰

- å¯¹æ¯ä¸ªä½ç½®ç‹¬ç«‹åº”ç”¨ä¸¤å±‚å…¨è¿æ¥ç½‘ç»œï¼ˆReLUæ¿€æ´»ï¼‰

```
FFN(xáµ¢) = max(0, xáµ¢ * Wâ‚ + bâ‚) * Wâ‚‚ + bâ‚‚
# Wâ‚ âˆˆ â„^(d_model Ã— d_ff)    bâ‚ âˆˆ â„^(d_ff)
# Wâ‚‚ âˆˆ â„^(d_ff Ã— d_model)    bâ‚‚ âˆˆ â„^(d_model)
```

## æ®‹å·®è¿æ¥ï¼ˆResidual Connectionï¼‰ä¸å±‚å½’ä¸€åŒ–ï¼ˆLayer Normalizationï¼‰

- æ®‹å·®è¿æ¥ç¼“è§£æ¢¯åº¦æ¶ˆå¤±ï¼Œå±‚å½’ä¸€åŒ–ç¨³å®šè®­ç»ƒ

```
X' = X + Sublayer(X)

LN(x'áµ¢) = Î³ * (x'áµ¢ - Î¼áµ¢) / Ïƒáµ¢ + Î²

Î¼áµ¢ = mean(x'áµ¢)
Ïƒáµ¢ = std(x'áµ¢)
Î³ âˆˆ â„^(d_model)
```

## ç¼–ç å™¨ï¼ˆEncoderï¼‰

- ç”±Nä¸ªç›¸åŒçš„ç¼–ç å™¨å±‚å †å è€Œæˆ
- ç¼–ç å™¨å±‚ï¼šå¤šå¤´è‡ªæ³¨æ„åŠ› --> æ®‹å·®&å½’ä¸€åŒ– --> FFN --> æ®‹å·®&å½’ä¸€åŒ–

```
Z' = LN(Z + MultiHeadAttention(Z, Z, Z))
EncoderOutput = LN(Z' + FFN(Z'))
```

## è§£ç å™¨ï¼ˆDecoderï¼‰

- ç”±Nä¸ªç›¸åŒçš„è§£ç å™¨å±‚å †å è€Œæˆ
- è§£ç å™¨å±‚ï¼šæ©ç è‡ªæ³¨æ„åŠ› --> æ®‹å·®&å½’ä¸€åŒ– --> ç¼–ç å™¨-è§£ç å™¨æ³¨æ„åŠ› --> æ®‹å·®&å½’ä¸€åŒ– --> FFN --> æ®‹å·®&å½’ä¸€åŒ–

```
D' = LN(D + MultiHeadAttention(D, D, D, Mask))
D'' = LN(D' + MultiHeadAttention(D', EncoderOutput, EncoderOutput))
DecoderOutput = LN(D'' + FFN(D''))
```

## è¾“å‡ºå±‚ï¼ˆLinearå±‚ä¸Softmaxï¼‰

- çº¿æ€§å±‚å°†è§£ç å™¨è¾“å‡ºæ˜ å°„åˆ°è¯è¡¨å¤§å°
- Softmaxç”Ÿæˆæ¯ä¸ªä½ç½®çš„è¯æ¦‚ç‡åˆ†å¸ƒ

```
Logits = DecoderOutput * W_vocab    # W_vocab âˆˆ â„^{n Ã— vocab_size}

P(w | context) = softmax(Logits[i])
```

***
ğŸ”™ [Go Back](README.md)
